{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigate the U-turn using Monte Carlo learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the start, please execute the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c8cfa434031df78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from racetrack_environment import RaceTrackEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46112ad628791ed0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the following cells to build a race track using the `RaceTrackEnv` as a test scenario.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab28c0c5fbe2404e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import build_uturn_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWWWWWWWWWWW\n",
      "WWWWW-oooooW\n",
      "WWWWW-oooooW\n",
      "WWWWW-oooooW\n",
      "WWWWWWWWWooW\n",
      "WWWWWWWWWooW\n",
      "WWWWW+oooooW\n",
      "WWWWW+oooooW\n",
      "WWWWW+oooooW\n",
      "WWWWWWWWWWWW\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "\n",
    "pos_map = track.course  # overlay track course\n",
    "plt.imshow(pos_map, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Race Track Environment\n",
    "Please make yourself familiar with the environment used in `race_track_environment.py` by testing it in the following cell.  \n",
    "In each state, the cell waits for a keyboard input and uses the typed number as the action.  \n",
    "If you type `c`, the loop will stop.  \n",
    "After each step, the environment writes the reward, and if the state terminates or truncates, it writes \"Episode ended. Resetting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last action: 1, reward:-1.0\n",
      "terminated: False, truncated: False, state: (2, 5, -1, 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input.\n",
      "Stopping control loop.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.ion()   # enable interactive plot\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "track.reset()\n",
    "track.render()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    key = input(\"Enter action 0-8 (c to stop): \")\n",
    "\n",
    "    if key.lower() == \"c\":\n",
    "        print(\"Stopping control loop.\")\n",
    "        break\n",
    "\n",
    "    # validate numeric input\n",
    "    if not key.isdigit():\n",
    "        print(\"Invalid input.\")\n",
    "        continue\n",
    "\n",
    "    action = int(key)\n",
    "\n",
    "    if action < 0 or action > 8:\n",
    "        print(\"Action must be between 0 and 8.\")\n",
    "        continue\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # step environment\n",
    "    next_state, reward, terminated, truncated, _ = track.step(action)\n",
    "    \n",
    "    print(f\"Last action: {action}, reward:{reward}\")\n",
    "    print(f\"terminated: {terminated}, truncated: {truncated}, state: {next_state}\")\n",
    "    #plt.clf() \n",
    "    track.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"***************************\")\n",
    "        print(\"Episode ended. Resetting.\")\n",
    "        print(\"***************************\")\n",
    "        track.reset()\n",
    "        track.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Direction_endcoding.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce1387b114d55595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Monte-Carlo-Based Prediction (Policy Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3672043edcf93af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a first-visit Monte-Carlo algorithm to evaluate the dummy policy as defined below on the U-turn course. The dummy policy turns the car to the right as soon as it stands in front of a wall. Try to understand how the policy works before you start to code.\n",
    "Think about what the different dimensions of the policy array encode. It might be helpful to print parts of the array for visualization.\n",
    "\n",
    "How can we interprete the state values resulting from the evaluation with first-visit Monte-Carlo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select course and initialize dummy policy\n",
    "\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_pi = np.ones([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY]) * 4 \n",
    "\n",
    "dummy_slow_pi[:track.bounds[0]//2, :, 0 , 0] = 5  # accelerate right\n",
    "dummy_slow_pi[:track.bounds[0]//2, -2:, 0, :] = 6  # accelerate bottom left\n",
    "dummy_slow_pi[-2:, track.bounds[1]//2:, :, 0] = 0  # accelerate top left\n",
    "\n",
    "pi = dummy_slow_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple and deterministic dummy policy will always guarantee the car to reach the finish line. Thus, the state values can be interpreted as the number of timesteps that is necessary to reach the goal from that specific state (i.e. position and velocity) if we are following the policy.\n",
    "\n",
    "In simple words, the policy acts as follows:\n",
    "- ```dummy_slow_pi[:track.bounds[0]//2, :, 0, 0] = 5```: This part of the policy accelerates the car to the right when it is located anywhere in the upper half of the track bounds (first two dimensions) and has no velocity, i.e. is standing still (last two dimensions). This means that this part of the policy accelerates the car to a maximum velocity of ```v_x = 1```. As the car is not affected by this after it is already moving.\n",
    "- ```dummy_slow_pi[:track.bounds[0]//2, -2:, 0, :] = 6```: This part of the policy takes effect when the car is in the upper half of the right boundary of the track. It only affects the car, when there is no vertical velocity (3rd dimension). As the car reaches this area of the space with the velocity ```v_x = 1``` and ```v_y = 0```, the velocity is ```v_x = 0``` and ```v_y = 1``` after this part of the policy was applied once. \n",
    "- ```dummy_slow_pi[-2:, track.bounds[1]//2:, :, 0] = 0```: This part of the policy takes effect when the car is at the right half of the lower boundary of the track. It only affects the car, when there is no horizontal velocity (4th dimension). As the car reaches this area of the space with the velocity ```v_x = 0``` and ```v_y = -1```, the velocity is ```v_x = -1``` and ```v_y = 0``` after this part of the policy was applied once.\n",
    "\n",
    "Overall, we can see that the absolute value of the car's velocity never goes above 1. While this makes the movement of the car manageable, this is not the fastest way to get through the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global setup for the tasks ---\n",
    "_course_dim = (10, 12)\n",
    "_inner_wall_dim = (6, 6) # Approximate inner wall dimensions resulting from 3-wide path\n",
    "\n",
    "# Select course and initialize dummy policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "\n",
    "dummy_slow_pi = np.ones([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY], dtype=int) * 4 \n",
    "dummy_slow_pi[:track.bounds[0]//2, :, 0 , 0] = 5  # accelerate right\n",
    "dummy_slow_pi[:track.bounds[0]//2, -2:, 0, :] = 6  # accelerate bottom left\n",
    "dummy_slow_pi[-2:, track.bounds[1]//2:, :, 0] = 0  # accelerate top left\n",
    "pi = dummy_slow_pi\n",
    "\n",
    "\n",
    "# --- Solutions ---\n",
    "\n",
    "def interact(pi, state):\n",
    "    \"\"\"Interact with the environment to get to the next state.\n",
    "\n",
    "    Args:\n",
    "        pi:  A deterministic stationary policy.\n",
    "            Type: numpy.ndarray\n",
    "            Shape: (10, 12, 11, 11)\n",
    "                • axis 0 → row coordinate of state      (0…9)\n",
    "                • axis 1 → column coordinate            (0…11)\n",
    "                • axis 2 → vertical velocity            (0…10)\n",
    "                • axis 3 → horizontal velocity          (0…10)\n",
    "            Entry type: integer action ∈ {0,1,…,8}\n",
    "        state: The current state before interaction\n",
    "            Type: tuple of 4 np.int8\n",
    "            Shape: (4,)\n",
    "            Contents: (row, col, v_row, v_col)\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        next_state: The next state after interaction\n",
    "        reward: The reward for the current interaction\n",
    "        terminated: If the goal was reached\n",
    "        truncated: If the boundary of the track was breached\n",
    "    \"\"\"\n",
    "    # Use the policy array to get the integer action for the current state\n",
    "    action_int = pi[state]\n",
    "    \n",
    "    # Convert integer action to tuple for the step function (optional if step handles ints, but safe)\n",
    "    # The provided step function handles ints via action_to_tuple check, but let's be explicit\n",
    "    action_tuple = track.action_to_tuple(action_int)\n",
    "    \n",
    "    # Interact with the environment\n",
    "    # Note: track.step returns 5 values (s, r, term, trunc, info)\n",
    "    next_state, reward, terminated, truncated, _ = track.step(action_tuple)\n",
    "    \n",
    "    return next_state, reward, terminated, truncated\n",
    "\n",
    "def gather_experience(pi, max_episode_len):\n",
    "    \"\"\"Simulate a full episode of data by repeatedly interacting with the environment.\n",
    "\n",
    "    End the episode when the finish line is reached. Whenever the car leaves the track, simply\n",
    "    reset the environment.\n",
    "    \n",
    "    Args:\n",
    "        pi: The policy to apply\n",
    "            Same indexing convention as in interact().\n",
    "        max_episode_len: The number of steps at which the episode is terminated automatically\n",
    "\n",
    "    Returns:\n",
    "        states: All states that were visited in the episode\n",
    "            Type: list of tuples\n",
    "            Length: T  (T ≤ max_episode_len)\n",
    "            Each element shape: (4,) tuple of np.int8\n",
    "\n",
    "            Example (max_episode_len=2):\n",
    "            states length = 2\n",
    "            states = [(3,5,0,0), (3,6,0,1)]\n",
    "            \n",
    "        rewards: All rewards that were acquired in the episode\n",
    "            Type: list of floats\n",
    "            Length: T\n",
    "            Each item shape: scalar float\n",
    "\n",
    "            Example: [-1.0, -1.0]\n",
    "            \n",
    "        visited_states: The unique states that were visited in the episode\n",
    "            Type: set of tuples\n",
    "            Each tuple: shape (4,), np.int8\n",
    "            Size: ≤ T   (unique states only)\n",
    "\n",
    "            Example:\n",
    "            {(3,5,0,0), (3,6,0,1)}\n",
    "            \n",
    "        first_visit_list: Whether it was the first time in the episode that the\n",
    "            state at the same index was visited\n",
    "            Type: list of booleans\n",
    "            Length: T\n",
    "            first_visit_list[t] = True if states[t] is first visit in episode\n",
    "\n",
    "            Example: [True, True]\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    rewards = []\n",
    "    visited_states = set()\n",
    "    first_visit_list = []\n",
    "    \n",
    "    # Reset environment\n",
    "    current_state = track.reset()\n",
    "    \n",
    "    # To track first visits within *this* episode\n",
    "    episode_visited_set = set()\n",
    "    \n",
    "    for k in range(max_episode_len):\n",
    "        \n",
    "        # Save momentary state\n",
    "        states.append(current_state)\n",
    "        \n",
    "        # Check for first_visit\n",
    "        if current_state not in episode_visited_set:\n",
    "            first_visit_list.append(True)\n",
    "            episode_visited_set.add(current_state)\n",
    "            visited_states.add(current_state) # Add to unique set of visited states return\n",
    "        else:\n",
    "            first_visit_list.append(False)\n",
    "        \n",
    "        # Interact\n",
    "        next_state, reward, terminated, truncated = interact(pi, current_state)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if terminated:\n",
    "            # Terminate if finish line passed\n",
    "            break\n",
    "            \n",
    "        if truncated:\n",
    "            # Reset environment if car left track, but continue episode logic\n",
    "            current_state = track.reset()\n",
    "        else:\n",
    "            current_state = next_state\n",
    "            \n",
    "    return states, rewards, visited_states, first_visit_list\n",
    "\n",
    "def learn(values, n_dict, states, rewards, first_visit_list, gamma):\n",
    "    \"\"\"Learn from the collected data using the incremental first-visit MC-based prediction algorithm.\n",
    "\n",
    "    Args:\n",
    "        values: The state-value estimates before the current update step\n",
    "        n_dict: The state visit counts before the current update step\n",
    "            Type: dict\n",
    "            Keys: states as 4-tuples\n",
    "            Values: int (visit counts)\n",
    "            \n",
    "        states: All states that were visited in the last episode\n",
    "            Type: list of tuples\n",
    "            Length: T   (episode length)\n",
    "            Each tuple: shape (4,), np.int8\n",
    "            \n",
    "        rewards: All rewards that were visited in the last episode\n",
    "            Type: list of floats\n",
    "            Length: T\n",
    "            \n",
    "        first_visit_list: Whether it was the first time in the episode that the\n",
    "            state at the same index was visited\n",
    "            Type: list of bool\n",
    "            Length: T\n",
    "            \n",
    "        gamma: Discount factor\n",
    "\n",
    "    Returns:\n",
    "        values: The updated state-value estimates\n",
    "            Same type and shape as input:\n",
    "            numpy.ndarray of shape (10,12,11,11)\n",
    "        n_dict: The state visit counts after the current update step\n",
    "            Updated dict with visit counts\n",
    "    \"\"\"\n",
    "    g = 0  \n",
    "    # Loop backwards through the episode\n",
    "    for state, reward, first_visit in zip(states[::-1], rewards[::-1], first_visit_list[::-1]): \n",
    "        # Calculate return (accumulate reward)\n",
    "        g = gamma * g + reward\n",
    "        \n",
    "        # Update values if it is the first visit in the episode\n",
    "        if first_visit:\n",
    "            # Count visits to this state in n_dict\n",
    "            if state not in n_dict:\n",
    "                n_dict[state] = 0\n",
    "            n_dict[state] += 1\n",
    "            \n",
    "            # Incremental mean update: V(s) <- V(s) + 1/N(s) * (G - V(s))\n",
    "            values[state] += (1.0 / n_dict[state]) * (g - values[state])\n",
    "            \n",
    "    return values, n_dict\n",
    "\n",
    "# --- Main Execution Loop ---\n",
    "\n",
    "# initialize the value function\n",
    "values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY])\n",
    "\n",
    "# initialize an empty dict to count the number of visits\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 500 # number of evaluated episodes\n",
    "max_episode_len = 2000 # number of allowed timesteps per episode\n",
    "\n",
    "print(\"Starting Monte-Carlo Policy Evaluation...\")\n",
    "for e in tqdm(range(no_episodes), position=0, leave=True):\n",
    "    states, rewards, visited_states, first_visit_list = gather_experience(pi, max_episode_len)\n",
    "    values, n_dict = learn(values, n_dict, states, rewards, first_visit_list, gamma)\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "def text_print_pos_map(_pos_map):\n",
    "    print(\"\\nValue Map (Position Only):\")\n",
    "    for row in _pos_map:\n",
    "        print(' '.join(['{:4d}'.format(int(r)) for r in row]))\n",
    "        \n",
    "def plot_pos_map(_pos_map):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(_pos_map, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(label='Negative Value (Brighter = High Cost)')\n",
    "    plt.title(\"State Values (min over velocities)\")\n",
    "    plt.show()\n",
    "\n",
    "# calculate minimum value with respect to velocities\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "for s_x in range(x_size):\n",
    "    for s_y in range(y_size):\n",
    "        # We take the minimum (most negative) value to represent the worst-case cost-to-go\n",
    "        pos_map[s_y, s_x] = np.min(values[s_y, s_x, :, :])\n",
    "        \n",
    "text_print_pos_map(pos_map)\n",
    "# Plotting -pos_map so that 0 (goal) is dark and high costs (far away) are bright\n",
    "plot_pos_map(-pos_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function templates to help structure the code\n",
    "# Note that the environment/track is a global variable here, so it does not need to be passed as an argument\n",
    "# This is not a good practice in general, but it is done here to simplify the code\n",
    "# You can therefore use the track variable directly in the functions\n",
    "# e.g., track.reset()\n",
    "\n",
    "def interact(pi, state):\n",
    "    \"\"\"Interact with the environment to get to the next state.\n",
    "\n",
    "    Args:\n",
    "        pi:  A deterministic stationary policy.\n",
    "            Type: numpy.ndarray\n",
    "            Shape: (10, 12, 11, 11)\n",
    "                • axis 0 → row coordinate of state      (0…9)\n",
    "                • axis 1 → column coordinate            (0…11)\n",
    "                • axis 2 → vertical velocity            (0…10)\n",
    "                • axis 3 → horizontal velocity          (0…10)\n",
    "            Entry type: integer action ∈ {0,1,…,8}\n",
    "        state: The current state before interaction\n",
    "            Type: tuple of 4 np.int8\n",
    "            Shape: (4,)\n",
    "            Contents: (row, col, v_row, v_col)\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        next_state: The next state after interaction\n",
    "        reward: The reward for the current interaction\n",
    "        terminated: If the goal was reached\n",
    "        truncated: If the boundary of the track was breached\n",
    "    \"\"\"\n",
    "    \n",
    "    action = track.action_to_tuple(pi[state])\n",
    "    next_state, reward, terminated, truncated, _ = track.step(action)\n",
    "\n",
    "    return next_state, reward, terminated, truncated\n",
    "\n",
    "\n",
    "def gather_experience(pi, max_episode_len):\n",
    "    \"\"\"Simulate a full episode of data by repeatedly interacting with the environment.\n",
    "\n",
    "    End the episode when the finish line is reached. Whenever the car leaves the track, simply\n",
    "    reset the environment.\n",
    "    \n",
    "    Args:\n",
    "        pi: The policy to apply\n",
    "            Same indexing convention as in interact().\n",
    "        max_episode_len: The number of steps at which the episode is terminated automatically\n",
    "\n",
    "    Returns:\n",
    "        states: All states that were visited in the episode\n",
    "            Type: list of tuples\n",
    "            Length: T  (T ≤ max_episode_len)\n",
    "            Each element shape: (4,) tuple of np.int8\n",
    "\n",
    "            Example (max_episode_len=2):\n",
    "            states length = 2\n",
    "            states = [(3,5,0,0), (3,6,0,1)]\n",
    "            \n",
    "        rewards: All rewards that were acquired in the episode\n",
    "            Type: list of floats\n",
    "            Length: T\n",
    "            Each item shape: scalar float\n",
    "\n",
    "            Example: [-1.0, -1.0]\n",
    "            \n",
    "        visited_states: The unique states that were visited in the episode\n",
    "            Type: set of tuples\n",
    "            Each tuple: shape (4,), np.int8\n",
    "            Size: ≤ T   (unique states only)\n",
    "\n",
    "            Example:\n",
    "            {(3,5,0,0), (3,6,0,1)}\n",
    "            \n",
    "        first_visit_list: Whether it was the first time in the episode that the\n",
    "            state at the same index was visited\n",
    "            Type: list of booleans\n",
    "            Length: T\n",
    "            first_visit_list[t] = True if states[t] is first visit in episode\n",
    "\n",
    "            Example: [True, True]\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    rewards = []\n",
    "    visited_states = set()\n",
    "    first_visit_list = []\n",
    "    \n",
    "    # Reset environment\n",
    "    current_state = track.reset()\n",
    "    \n",
    "    # To track first visits within *this* episode\n",
    "    episode_visited_set = set()\n",
    "    \n",
    "    for k in range(max_episode_len):\n",
    "        \n",
    "        # Save momentary state\n",
    "        states.append(current_state)\n",
    "        \n",
    "        # Check for first_visit\n",
    "        if current_state not in episode_visited_set:\n",
    "            first_visit_list.append(True)\n",
    "            episode_visited_set.add(current_state)\n",
    "            visited_states.add(current_state) # Add to unique set of visited states return\n",
    "        else:\n",
    "            first_visit_list.append(False)\n",
    "        \n",
    "        # Interact\n",
    "        next_state, reward, terminated, truncated = interact(pi, current_state)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if terminated:\n",
    "            # Terminate if finish line passed\n",
    "            break\n",
    "            \n",
    "        if truncated:\n",
    "            # Reset environment if car left track, but continue episode logic\n",
    "            current_state = track.reset()\n",
    "        else:\n",
    "            current_state = next_state\n",
    "            \n",
    "    return states, rewards, visited_states, first_visit_list\n",
    "\n",
    "def learn(values, n_dict, states, rewards, first_visit_list, gamma):\n",
    "    \"\"\"Simulate a full episode of data by repeatedly interacting with the environment.\n",
    "\n",
    "    End the episode when the finish line is reached. Whenever the car leaves the track, simply\n",
    "    reset the environment.\n",
    "    \n",
    "    Args:\n",
    "        pi: The policy to apply\n",
    "            Same indexing convention as in interact().\n",
    "        max_episode_len: The number of steps at which the episode is terminated automatically\n",
    "\n",
    "    Returns:\n",
    "        states: All states that were visited in the episode\n",
    "            Type: list of tuples\n",
    "            Length: T  (T ≤ max_episode_len)\n",
    "            Each element shape: (4,) tuple of np.int8\n",
    "\n",
    "            Example (max_episode_len=2):\n",
    "            states length = 2\n",
    "            states = [(3,5,0,0), (3,6,0,1)]\n",
    "            \n",
    "        rewards: All rewards that were acquired in the episode\n",
    "            Type: list of floats\n",
    "            Length: T\n",
    "            Each item shape: scalar float\n",
    "\n",
    "            Example: [-1.0, -1.0]\n",
    "            \n",
    "        visited_states: The unique states that were visited in the episode\n",
    "            Type: set of tuples\n",
    "            Each tuple: shape (4,), np.int8\n",
    "            Size: ≤ T   (unique states only)\n",
    "\n",
    "            Example:\n",
    "            {(3,5,0,0), (3,6,0,1)}\n",
    "            \n",
    "        first_visit_list: Whether it was the first time in the episode that the\n",
    "            state at the same index was visited\n",
    "            Type: list of booleans\n",
    "            Length: T\n",
    "            first_visit_list[t] = True if states[t] is first visit in episode\n",
    "\n",
    "            Example: [True, True]\n",
    "    \"\"\"\n",
    "    g = 0  \n",
    "    # Loop backwards through the episode\n",
    "    for state, reward, first_visit in zip(states[::-1], rewards[::-1], first_visit_list[::-1]): \n",
    "        # Calculate return (accumulate reward)\n",
    "        g = gamma * g + reward\n",
    "        \n",
    "        # Update values if it is the first visit in the episode\n",
    "        if first_visit:\n",
    "            # Count visits to this state in n_dict\n",
    "            if state not in n_dict:\n",
    "                n_dict[state] = 0\n",
    "            n_dict[state] += 1\n",
    "            \n",
    "            # Incremental mean update: V(s) <- V(s) + 1/N(s) * (G - V(s))\n",
    "            values[state] += (1.0 / n_dict[state]) * (g - values[state])\n",
    "            \n",
    "    return values, n_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac5467fab5f148f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 540.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize the value function\n",
    "values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY])\n",
    "\n",
    "# initialize an empty dict to count the number of visits\n",
    "# note that in the lecture the list l(x_k) was used for this purpose\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 500 # number of evaluated episodes\n",
    "max_episode_len = 2000 # number of allowed timesteps per episode\n",
    "\n",
    "for e in tqdm(range(no_episodes), position=0, leave=True):\n",
    "\n",
    "    states, rewards, visited_states, first_visit_list = gather_experience(pi, max_episode_len)\n",
    "\n",
    "    values, n_dict = learn(values, n_dict, states, rewards, first_visit_list, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fe53fdd68a6c909",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To visualize the result of the evaluation, plot the state values as a function of **position only** (so that you get a two dimensional representation of the state value) and in the form of a tabular represenation and a heatmap. In order to omit dependence of the velocity dimensions, use the minimum of the value function with respect to the velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74fc6bcd5def8261",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 000 000 000 000 000 000 000 000 000 000 000\n",
      "000 000 000 000 000 -17 -16 -15 -14 -13 -12 000\n",
      "000 000 000 000 000 -16 -15 -14 -13 -12 -11 000\n",
      "000 000 000 000 000 -15 -14 -13 -12 -11 -10 000\n",
      "000 000 000 000 000 000 000 000 000 000 -09 000\n",
      "000 000 000 000 000 000 000 000 000 000 -08 000\n",
      "000 000 000 000 000 000 000 000 000 000 -07 000\n",
      "000 000 000 000 000 000 000 000 000 000 -06 000\n",
      "000 000 000 000 000 000 -01 -02 -03 -04 -05 000\n",
      "000 000 000 000 000 000 000 000 000 000 000 000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_print_pos_map(_pos_map):\n",
    "    for row in _pos_map:\n",
    "        print(' '.join(x_size*['{}']).format(*[str(int(r)).zfill(3) for r in row]))\n",
    "        \n",
    "def plot_pos_map(_pos_map):\n",
    "    plt.imshow(_pos_map, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# calculate minimum value with respect to velocities\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "\n",
    "for s_x in range(x_size):\n",
    "    for s_y in range(y_size):\n",
    "        pos_map[s_y, s_x] = np.min(values[s_y, s_x, :, :])\n",
    "        \n",
    "text_print_pos_map(pos_map)\n",
    "plot_pos_map(-pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54642e38ce9d8a67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) On-Policy $\\varepsilon$-Greedy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a81f379107be8dd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Starting with the previously used turn-right-if-wall dummy policy, write an on-policy Monte-Carlo based first-visit $\\varepsilon$-greedy control algorithm to solve the U-turn course. The policy is now stochastic: it does not contain simple action commands for each state, but probabilities for each possible action. Again, please make sure to understand how the stochastic policy works before coding.\n",
    "\n",
    "\n",
    "Make sure to implement an upper bound for episode length (we suggest a boundary of 200 steps). Why do we need a bound like this? What happens to the state values / state-action values if we increase the bound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "\n",
    "dummy_slow_stoch_pi = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 9])\n",
    "\n",
    "dummy_slow_stoch_pi[  :,   :, :, :, 4] = 1 # set probability of doing nothing to one for every state\n",
    "\n",
    "# set probability to accelerate right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 5] = 1 \n",
    "# set probability to do nothing where we want to accelerate right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 6] = 1 # probability to accelerate bottom left\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 0] = 1 # probability to accelerate top left\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 4] = 0\n",
    "\n",
    "pi = dummy_slow_stoch_pi       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89a131cffdbb5d52",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Algorithm given below.\n",
    "\n",
    "As we can see, the dummy policy allows for the initial episode to be solved very fast. After that, the dummy policy is forgotten and it takes some time until the agent is able to solve the problem again. \n",
    "\n",
    "The limitation of the episode length forces the agent to learn at least after the allowed number of steps were taken. If one would increase the limit, this would mainly inflate the accumulated return, resulting in larger negative action values for the visited states. As long as we do NOT find the goal, action values will correlate with the time limit. If we find the goal reproducible, the action values will drift towards their true optimal value independently from the time limit.\n",
    "\n",
    "If we do not implement a time limit and allow the episode to terminate only by reaching the goal, the accumulated negative return will explode (we will get very large numbers). As we try to act greedy (take the highest rated and not the lowest rated action), low action values would suggest that the goal is not to be found on the path taken previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function templates, some of these are quite close to the solutions for task 1\n",
    "\n",
    "def policy(pi, state, deterministic, epsilon):\n",
    "    \"\"\"Selects an action based on the current state and policy.\n",
    "    \n",
    "    Args:\n",
    "        pi: The current stochastic policy. For each state, this is a\n",
    "            probability vector over 9 possible actions.\n",
    "            Type: numpy.ndarray\n",
    "            Shape: (10, 12, 11, 11, 9)\n",
    "                • axis 0 → row coordinate of the state          (0…9)\n",
    "                • axis 1 → column coordinate                    (0…11)\n",
    "                • axis 2 → vertical velocity                     (0…10)\n",
    "                • axis 3 → horizontal velocity                   (0…10)\n",
    "                • axis 4 → action probabilities for all actions 0…8  (0…1)\n",
    "                \n",
    "        state: The state vector.\n",
    "            Current state (y, x, v_y, v_x).\n",
    "            Shape: (4,)\n",
    "            \n",
    "        deterministic: If True, actions are selected according to the policy's\n",
    "            highest probability (greedy). If False, actions are selected\n",
    "            using ε‑greedy exploration.\n",
    "        epsilon: Probability of selecting a random action when using ε‑greedy.\n",
    "    \n",
    "    Returns:\n",
    "        action (int): The selected action (integer in range 0–8).\n",
    "    \"\"\"\n",
    "    # We use explicit epsilon-greedy logic here. \n",
    "    # If the policy 'pi' is already epsilon-soft (from learn), argmax gets the greedy action.\n",
    "    # If the policy 'pi' is deterministic (initial dummy), epsilon check ensures exploration.\n",
    "    \n",
    "    if deterministic:\n",
    "        # Greedy selection\n",
    "        action = np.argmax(pi[state])\n",
    "    else:\n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 8)\n",
    "        else:\n",
    "            action = np.argmax(pi[state])\n",
    "            \n",
    "    return action\n",
    "\n",
    "def interact(pi, state, deterministic, epsilon):\n",
    "    \"\"\"Interact with the environment to get to the next state. Either follow\n",
    "    the given policy or explore randomly with probability epsilon.\n",
    "\n",
    "    Args:\n",
    "        pi (numpy.ndarray):\n",
    "            The policy to follow\n",
    "            Stochastic policy.\n",
    "            Shape: (10, 12, 11, 11, 9)\n",
    "            \n",
    "        state (tuple[int, int, int, int]):\n",
    "            The current state before interaction (y, x, v_y, v_x)\n",
    "        \n",
    "        deterministic (bool):\n",
    "            Whether actions are chosen deterministically or eps-greedily\n",
    "        epsilon (float):\n",
    "            The probability for random interaction\n",
    "\n",
    "    Returns:\n",
    "        next_state: The next state after interaction\n",
    "        reward (float):\n",
    "            The reward for the current interaction\n",
    "            Shape: scalar\n",
    "            \n",
    "        terminated (bool):\n",
    "            If the goal was reached\n",
    "            \n",
    "        truncated (bool):\n",
    "            If the boundary of the track was breached\n",
    "            \n",
    "        action (int):\n",
    "            The applied action (0–8).\n",
    "            Shape: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    action = policy(pi, state, deterministic, epsilon)\n",
    "    \n",
    "    # Step environment\n",
    "    # Note: track.step accepts integer action (0-8)\n",
    "    next_state, reward, terminated, truncated, _ = track.step(action)\n",
    "    \n",
    "    return next_state, reward, terminated, truncated, action\n",
    "\n",
    "def gather_experience(pi, max_episode_len, deterministic=False, epsilon=0.1):\n",
    "    \"\"\"Simulate a full episode of data by repeatedly interacting with the environment.\n",
    "\n",
    "    End the episode when the finish line is reached. Whenever the car leaves the track, simply\n",
    "    reset the environment.\n",
    "\n",
    "    Args:\n",
    "        pi (numpy.ndarray):\n",
    "            The policy to apply\n",
    "            Stochastic policy.\n",
    "            Shape: (10, 12, 11, 11, 9)\n",
    "\n",
    "        max_episode_len (int):\n",
    "            The number of steps at which the episode is terminated automatically\n",
    "        \n",
    "        deterministic (bool):\n",
    "            Whether actions are chosen deterministically or eps-greedily\n",
    "            \n",
    "        epsilon (float):\n",
    "            Exploration probability\n",
    "\n",
    "    Returns:\n",
    "        state_actions (list[tuple[int, int, int, int, int, int]]):\n",
    "            All states that were visited and all actions that were applied in \n",
    "            the episode, states and actions are simply concatenated. **HINT**: You can use \n",
    "            ```track.state_action(state, action)``` to concatenate state and action.\n",
    "            Concatenated state-action tuples.\n",
    "            One entry per time step k in the episode.\n",
    "            Each tuple:\n",
    "                (y, x, v_y, v_x, a_y_idx, a_x_idx)\n",
    "            Length: episode length ≤ max_episode_len\n",
    "            Shape of each element: (6,)\n",
    "            \n",
    "        rewards (list[float]):\n",
    "            All rewards that were acquired in the episode\n",
    "            Length: episode length\n",
    "            \n",
    "        visited_states (set[tuple[int, int, int, int]]:\n",
    "            The unique states that were visited in the episode\n",
    "            Set of unique visited states.\n",
    "            Each element shape: (4,)\n",
    "            \n",
    "        first_visit_list (list[bool]):\n",
    "            Whether it was the first time in the episode that the\n",
    "            state at the same index was visited\n",
    "            Length: episode length\n",
    "            \n",
    "        pos_map (numpy.ndarray):\n",
    "            A map of the track where all state visits are marked\n",
    "            A visit-count map marking each visited (y,x) cell.\n",
    "            Shape: (10, 12)\n",
    "            Example: pos_map:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                                [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize variables in which collected data will be stored\n",
    "    state_actions = [] # list of tuples\n",
    "    rewards = [] # list of floats\n",
    "    visited_states = set() # set of tuples (states)\n",
    "    visited_state_actions_in_episode = set() # helper for first_visit_list\n",
    "    first_visit_list = [] # list of booleans\n",
    "    \n",
    "    x_size, y_size = track.bounds[1], track.bounds[0]\n",
    "    pos_map = np.zeros((y_size, x_size)) # initializes a map that can be plotted\n",
    "        \n",
    "    state = track.reset()\n",
    "    \n",
    "    for k in range(max_episode_len):\n",
    "        pos_map[state[0], state[1]] += 1  # mark the visited position on the map\n",
    "        visited_states.add(state)\n",
    "        \n",
    "        # Interact\n",
    "        next_state, reward, terminated, truncated, action = interact(pi, state, deterministic, epsilon)\n",
    "        \n",
    "        # Store State-Action pair info\n",
    "        # Get full index tuple for Q-table: (y, x, vy, vx, ay, ax)\n",
    "        sa_tuple = track.state_action(state, action)\n",
    "        state_actions.append(sa_tuple)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Check first visit (Standard MC Control checks first visit of (S,A))\n",
    "        if sa_tuple not in visited_state_actions_in_episode:\n",
    "            first_visit_list.append(True)\n",
    "            visited_state_actions_in_episode.add(sa_tuple)\n",
    "        else:\n",
    "            first_visit_list.append(False)\n",
    "            \n",
    "        if terminated:\n",
    "            break\n",
    "            \n",
    "        if truncated:\n",
    "            # Reset environment but continue episode (as per instructions)\n",
    "            state = track.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    return state_actions, rewards, visited_states, first_visit_list, pos_map\n",
    "\n",
    "def learn(pi, action_values, n_dict, state_actions, rewards, first_visit_list, gamma, epsilon):\n",
    "    \"\"\"Learn from the collected data with eps-greedy MC-control and update the policy.\n",
    "\n",
    "    Args:\n",
    "        pi (numpy.ndarray):\n",
    "            The policy before the update step\n",
    "            Shape: (10, 12, 11, 11, 9)\n",
    "            \n",
    "        action_values (numpy.ndarray):\n",
    "            The action-value estimates Q(s,a) before the current update step\n",
    "            Stored using the 3×3 acceleration grid.\n",
    "            Shape: (10, 12, 11, 11, 3, 3)\n",
    "            \n",
    "        n_dict (dict):\n",
    "            The state action visit counts before the update\n",
    "            Counter of how many times each (state, action) pair was visited.\n",
    "            Keys: (y, x, v_y, v_x, a_y_idx, a_x_idx)\n",
    "            Key shape: (6,)\n",
    "            Values: int counts\n",
    "            \n",
    "        state_actions (list[tuple[int,int,int,int,int,int]]):\n",
    "            All state actions that were done in the last epsiode\n",
    "            Full sequence of state-action pairs from the last episode.\n",
    "            Length = episode length ≤ max_episode_len\n",
    "            Each element shape = (6,)\n",
    "        \n",
    "        rewards:\n",
    "            All rewards that were visited in the last episode\n",
    "            Length = episode length\n",
    "            \n",
    "        first_visit_list:\n",
    "            Whether it was the first time in the episode that the\n",
    "            state at the same index was visited\n",
    "            Length = episode length\n",
    "            \n",
    "        gamma (float):\n",
    "            Discount factor\n",
    "            \n",
    "        epsilon (float):\n",
    "            Exploration probability\n",
    "\n",
    "    Returns:\n",
    "        pi:\n",
    "            The updated policy\n",
    "            Shape: (10, 12, 11, 11, 9)\n",
    "            \n",
    "        action_values:\n",
    "            The updated action-value estimates\n",
    "            Updated Q(s,a) array.\n",
    "            Shape: (10, 12, 11, 11, 3, 3)\n",
    "            \n",
    "        n_dict: The updated state action visit counts\n",
    "             Same structure as input.\n",
    "        \n",
    "    \"\"\"\n",
    "    g = 0\n",
    "    \n",
    "    # Iterate backwards\n",
    "    for i in range(len(state_actions) - 1, -1, -1):\n",
    "        sa = state_actions[i]\n",
    "        reward = rewards[i]\n",
    "        is_first_visit = first_visit_list[i]\n",
    "        \n",
    "        g = gamma * g + reward\n",
    "        \n",
    "        if is_first_visit:\n",
    "            # Update counts\n",
    "            if sa not in n_dict:\n",
    "                n_dict[sa] = 0\n",
    "            n_dict[sa] += 1\n",
    "            \n",
    "            # Update Action Values (Q)\n",
    "            # sa is (y, x, vy, vx, ay_idx, ax_idx)\n",
    "            # action_values is indexed by [y, x, vy, vx, ay_idx, ax_idx]\n",
    "            current_q = action_values[sa]\n",
    "            action_values[sa] += (1.0 / n_dict[sa]) * (g - current_q)\n",
    "            \n",
    "            # Update Policy (Epsilon-Greedy Update)\n",
    "            # State part of tuple is sa[:4]\n",
    "            state_idx = sa[:4]\n",
    "            \n",
    "            # Find best action for this state\n",
    "            # action_values[state_idx] is a 3x3 array\n",
    "            q_vals_state = action_values[state_idx]\n",
    "            best_action_flat = np.argmax(q_vals_state) # 0-8\n",
    "            \n",
    "            # Update probabilities\n",
    "            # Set all to epsilon / |A|\n",
    "            pi[state_idx] = epsilon / 9.0\n",
    "            # Add (1 - epsilon) to the best action\n",
    "            pi[state_idx][best_action_flat] += (1.0 - epsilon)\n",
    "\n",
    "    return pi, action_values, n_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9568aa87f2614759",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 100%|██████████| 5000/5000 [00:11<00:00, 428.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize action_values and counting dict\n",
    "action_values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 3, 3])\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "epsilon = 0.1 # exploration probability\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 5000 # number of evaluated episodes\n",
    "max_episode_len = 200 # number of evaluated timesteps per episode\n",
    "track_maps_l = []  # placeholder for tracks\n",
    "\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "for e in tqdm(range(no_episodes), desc='episode', mininterval=2):\n",
    "      \n",
    "    state_actions, rewards, visited_states, first_visit_list, pos_map = gather_experience(pi, max_episode_len, epsilon)\n",
    "\n",
    "    pi, action_values, n_dict = learn(pi, action_values, n_dict, state_actions, rewards, first_visit_list, gamma, epsilon)\n",
    "\n",
    "    # optional value map logging\n",
    "    track_maps_l.append(track.course + (pos_map > 0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef5799678637f070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeIAAAGdCAYAAADOnXC3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHLJJREFUeJzt3Xtc1HW+x/E3cnFXHXTLoyReNs1L2vEGWplpabTVdrzkHk2PxVbmRmWZtSq6pi2VWq14CTfb0pJjpo89m61rluVampdcvNQRpFMiaKAYaQ4KOii/84c6NYLJoPAZh9fz8fg+HvHjN/w+jDqvfsMMvxBJjgAAgIla1gMAAFCTEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAyFWRy0SZMmKiwstDg0AADVwuVyKS8v77z7VXuImzRpotzc3Oo+LAAA1S46Ovq8Ma72EJ85E74qOlpHOCsGAAShei6Xvs7NrdCzvyZPTUvSkcJCnp4GANR4vFgLAABDhBgAAEOEGAAAQ4QYAABDhBgAAEOEGAAAQ4QYAABDhBgAAEOEGAAAQ4QYAABDhBgAAEOVCnFCQoKysrJUXFystLQ09ezZ82LPBQBAjeB3iAcPHqyZM2fqueeeU5cuXbRu3TqtXLlSzZo1q4r5AAAIaiGSHH9usGnTJm3dulUPP/ywd1tGRoaWLVumCRMmnPf2LpdLbrdbUZGRXH0JABCUXC6X9rvdiqxA6/w6Iw4PD1dMTIxWrVrls33VqlXq0aNHubeJiIiQy+XyWQAA4BS/QtywYUOFhYUpPz/fZ3t+fr6ioqLKvU1iYqLcbrd35ebmVn5aAACCTKVerOU4vs9mh4SElNl2xtSpUxUZGeld0dHRlTkkAABBKcyfnQsKCnTixIkyZ7+NGjUqc5Z8hsfjkcfjqfyEAAAEMb/OiEtKSrRlyxbFxcX5bI+Li9OGDRsu6mAAANQEfp0RS9KMGTOUmpqqtLQ0bdy4USNHjlTz5s31yiuvVMV8AAAENb9DvHTpUl1++eV6+umndcUVV2jHjh264447tGfPnqqYDwCAoOb3+4gvFO8jBgAEuyp7HzEAALi4CHE1ydi9W488/rj1GACAABN0IZ63YIHefucd78cr16zRC8nJ1Xb84fHxyj10qMz2Xt26af6rr1bbHC1btdJ+t7vcWUY+/LC2ZGSooKhI2zIzNeyee8rs0/+uu5SWnq6Dx44pLT1d/zFgQJl9HkxIUHpWlr4rLtanaWnqwcU/AMBvQRfiqhIeHn5Bty8oKFBxcfFFmuanhYWF6Y3Fi7Vh3boynxvx0EN6ZupUPT9limI7dNBzkydrRkqKbr/zTu8+3a+7TguXLNHbqam6rlMnvZ2aqtSlSxXbvbt3n0GDB+uFmTP1wnPPqUeXLtqwbp3eWblSTbn4BwD4JahDPG/BAvW66SY9Mnq0jjqOjjqOmrdoIUlqd/XV+tuKFcovLNTu/fv12sKFuvzyy723Xblmjf40Z46m/elPyvn2Wy3/8ENJ0qgnntDmL77QgSNH9OWePUpOSVHdunUlSTf27q15b7yhBg0aeI83YfJkSWWfmm7arJmWLFum/MJC7Tt8WAuXLFGjRo28n58webI2btumocOHK2P3buV9/73eWLxY9erVO+/3PfnZZ/V/mZn629KlZT439J57NH/ePP3P0qXK3r1bf12yRAtff11jxo3z7vPI6NH654cf6qVp0/R/X36pl6ZN08erV+vR0aO9+4waM0Zvvv663nz9dX2ZmamxTzyhb/bu1YMJCRX5owEAnBbUIf79449r04YNmv/qq2oZFaWWUVH6Zu9eRUVF6f1PPtEX27frxthYDbjtNjVq3FipZ4Xrv+LjdeLECd1yww167He/kySVlpbqqcceU7drrtHI+Hj17tNHz77wgiRp04YN+v3jj+vw4cPe48166aVyZ1uybJl+cdll+lXv3vqPuDi1bNVKby5Z4rPPla1a6c4BA/SbO+/Ub+68Uzf27q0nx4//ye+59803a+B//qeeeOSRcj9fu3ZtHTt2zGdbcXGxYrt3V1jYqXezXXv99Vp91oU9PvrgA117+sIe4eHh6hITU2aff65a5d0HAFAxfr+P+FLidrvl8XhUXFTk8ys4RyQk6POtWzVl4kTvtofuv19fffONrmrdWl9/9ZUkKevrr/WHH50pSlLKrFne/87JzlbSpEma+ec/64lHHlFJSYnchw/LcZxz/spPSepzyy26pmNHtb/ySuV+882pme65R1syMtQ1NlZb09IkSbVq1dLvfvtbHTlyRJK0ODVVN/Xtq2f+8Idyv+5ll12meW+8oQeGDz/ny+U/+uAD/XbECC1ftkzbt25Vl5gY3Xv//YqIiFDDhg21f/9+NY6K0oGz5j+Qn6/Gp3+16eWnL/5x9j75+fm65RwX/wAAlC+oQ3wuXWJi1Ovmm5VfTqxatmrlDfGZIP5Yr5tu0u8nTFC79u3lioxUWFiYfv7zn6tOnToqKiqq0PHbXn21vtm71xthScrcuVOHDh1Su6uv9h43JzvbG2FJ2r9vn/7tR09fn+3lv/xFS996S+vL+dnwGdOSktQ4Kkofb9qkkJAQHcjP13+/8YbGjBunkydPeveryIU9/Ln4BwCgfEH91PS51KpVS+8tX67rO3f2Wf9+1VX6dO1a735FR4/63K5Z8+b623vvKWPHDg0bNEg9Y2I05vRTwP68mOtcwTp7+4mSEp/PO46jWrXO/UfWu08fPf7UUzpcUqLDJSWa+/rratCggQ6XlOje++6TJB07dkwJDzyghnXq6Opf/lJtmzdXTna23G63CgoKJEn5p8+Kf+zfGjXyngF/d/riH2fv0+hH+wAAKiboz4hLPB7VCg312bZ961b1HzRIOdnZPmeB59M1NlZhYWEa/+ST3mDeNXiwzz4ej0ehZx3vbJkZGWrWvLmimzb1nhW3u/pqNWjQQJk7d1Z4nrP1uf56n2P/un9/jRk3Tn179FDeWdeBPnHihHfbb+6+W+//4x/e7+mzjRvVJy5OL8+c6d2/76236rPTF/YoKSnRti1b1CcuTsuXLfPuc3NcnFa8+26l5weAmijoQ5yTna1u116r5i1a6OiRIzp48KDmpaTovgcf1BuLF2vmiy/qu4ICtbrqKv3m7rv1yIMPqrS0tNyvtXvXLoWHhyth1KhTZ9Q33KARDz1U5ngul0s39emj//38cxUVFZV529I/P/pIO774QvMXLdK40aMVGhammXPnau3HH2vbli2V/l6/zMz0+bhrbKxKS0uVkZ7u3XZV69aK7d5d//rsMzX4xS80aswYtT/9wrMz5s6apVVr12rM2LH6x7vv6s7+/XXzLbfolh+9T3jOjBl6LTVV29LS9NnGjbp/5Eg1a95cr3HxDwDwS9CHeNZLL+nVN9/UlowM1Tn9dOyenBz1veEGJU2frnc/+EC1a9fWnpwcffT+++eMsCR98fnnGvfEExozbpyemTpV69eu1eTERL2Wmurd57ONG/WXP/9Zby5ZooYNG+q5KVP0/DPPlPlaQwYM0J/mzNEHa9eqtLRUH77/vp4aNapK7oMfCw0N1WNPPqnWbduqpKREa9esUd8ePbQnJ8fne4i/+249/eyzmpSUpKxdu3TvkCFK27zZu8//LF2qyy6/XOOfflpRV1yhjB07dNcdd2gvF/+QJB39zHqCcnTn5/cIbnVDQqxHqBQu+gBUAUIMVL9ACjEXfQAA4BJBiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMBRmPQAQjA5caz1BWY0+C7EeAcGku2M9QdDgjBgAAEOEGAAAQ4QYAABDhBgAAEOEGAAAQ4QYAABDhBgAAEOEGAAAQ4QYAABDhBgAAEOEGAAAQ4QYAABDhBgAAEOEGAAAQ36FePz48dq8ebPcbrfy8/P1zjvvqE2bNlU1GwAAQc+vEPfu3VspKSm67rrrFBcXp7CwMK1atUp16tSpqvkAAAhqYf7sfPvtt/t8fN999+nbb79VTEyM1q1bd1EHAwCgJvArxGerX7++JOngwYPn3CciIkK1a9f2fuxyuS7kkAAABJULerHWjBkztG7dOqWnp59zn8TERLndbu/Kzc29kEMCABBUKh3il19+WR07dtTQoUN/cr+pU6cqMjLSu6Kjoyt7SAAAgk6lnpqePXu2+vXrp169ep33DNfj8cjj8VRqOAAAgp3fIZ4zZ44GDhyom266SdnZ2VUwEgAANYdfIU5JSdGwYcPUv39/FRYWqnHjxpKkw4cP69ixY1UyIAAAwcyvnxE//PDDatCggT755BPt37/fu4YMGVJV8wEAENT8OiMOCQmpqjkAAKiR+F3TAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGAqzHgAIRldaD1Cea60HQGUd/cx6AlQlzogBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMHRBIR4/frwcx1FycvLFmgcAgBql0iGOjY3VyJEj9fnnn1/MeQAAqFEqFeK6detq0aJFevDBB3Xo0KGLPRMAADVGpUKckpKiFStWaPXq1efdNyIiQi6Xy2cBAIBTwvy9wZAhQ9S1a1d169atQvsnJiZqypQp/h4GAIAawa8z4qZNm2rWrFkaPny4jh8/XqHbTJ06VZGRkd4VHR1dqUEBAAhGfp0Rx8TEqHHjxtqyZcsPXyAsTL169dKjjz6q2rVrq7S01Oc2Ho9HHo/n4kwLAECQ8SvEq1ev1jXXXOOzbcGCBcrMzNT06dPLRBgAAPw0v0J85MgRpaen+2w7evSovvvuuzLbAQDA+fGbtQAAMOT3q6bPdvPNN1+MOQAAqJE4IwYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMAQIQYAwBAhBgDAECEGAMBQmPUAQDA66jjWI5S1OcR6AlRWd/4+BTPOiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAw5HeImzRpotTUVBUUFOjo0aPatm2bunbtWhWzAQAQ9Py6HnGDBg20fv16rVmzRrfffrsOHDigVq1a6fvvv6+i8QAACG5+hXjcuHHau3ev7r//fu+2nJyciz4UAAA1hV9PTffr109paWlaunSp8vPztXXrVo0YMeInbxMRESGXy+WzAADAKX6FuGXLlkpISNBXX32lX/3qV3rllVc0e/Zs3XPPPee8TWJiotxut3fl5uZe8NAAAASLEElORXc+fvy40tLSdMMNN3i3zZo1S926dVOPHj3KvU1ERIRq167t/djlcik3N1dRkZEqLCys/ORAADvqVPifVfXZHGI9ASqrO3+fKqLutdYT/MDlcmm/263ICrTOrzPiffv2KSMjw2fbzp071bx583PexuPxqLCw0GcBAIBT/Arx+vXr1bZtW59tbdq04QVbAABUkl8hTk5O1nXXXafExES1atVKQ4cO1ciRI5WSklJV8wEAENT8CnFaWpoGDhyooUOHaseOHZo0aZJGjx6tt956q6rmAwAgqPn1PmJJWrFihVasWFEVswAAUOPwu6YBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMOT375oGgIuGC95XTCDOhIuGM2IAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMhVkPAKAG2xxiPQFgjjNiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAwRYgAADBFiAAAMEWIAAAz5FeLQ0FAlJSUpKytLRUVF2rVrlyZNmqSQEC5lBgBAZfh1PeJx48bpoYceUnx8vNLT0xUbG6sFCxbo8OHDmj17dlXNCABA0PIrxNdff73effddvffee5KknJwcDR06VLGxsVUyHAAAwc6vp6Y//fRT9e3bV61bt5YkdezYUT179vSGuTwRERFyuVw+CwAAnOLXGfH06dNVv359ZWZm6uTJkwoNDdXEiRP19ttvn/M2iYmJmjJlyoXOCQBAUPLrjHjIkCEaPny4hg0bpq5duyo+Pl5PPfWU7r333nPeZurUqYqMjPSu6OjoCx4aAIBg4dcZ8Ysvvqhp06ZpyZIlkqQdO3aoRYsWSkxM1MKFC8u9jcfjkcfjufBJAQAIQn6dEdepU0elpaU+206ePKlatXg7MgAAleHXGfHy5cs1ceJE7dmzR+np6erSpYvGjBmj+fPnV9V8AAAENb9CPGrUKCUlJWnu3Llq1KiR8vLyNG/ePP3xj3+sqvkAAAhqIZKc6jygy+WS2+1WVGSkCgsLq/PQQLU56lTrP6uK2cxvwENwq3ut9QQ/cLlc2u92K7ICreOHuwAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAYIsQAABgixAAAGCLEAAAY8uuiDwAqpm4Iv9cZQMVwRgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAIUIMAIAhQgwAgCFCDACAoTCrA9dzuawODQBAlfKncdUeYtfp4b7Oza3uQwMAUK1cLpcKCwt/cp8QSU71jPODJk2anHew83G5XMrNzVV0dPQFf61gxv1UMdxPFcP9VDHcTxUT7PeTy+VSXl7eefczeWq6IoNVVGFhYVD+AV5s3E8Vw/1UMdxPFcP9VDHBej9V9HvixVoAABgixAAAGLpkQ3z8+HFNmTJFx48ftx4loHE/VQz3U8VwP1UM91PFcD+dYvJiLQAAcMole0YMAEAwIMQAABgixAAAGCLEAAAYumRDnJCQoKysLBUXFystLU09e/a0HimgjB8/Xps3b5bb7VZ+fr7eeecdtWnTxnqsgDZ+/Hg5jqPk5GTrUQJOkyZNlJqaqoKCAh09elTbtm1T165drccKKKGhoUpKSlJWVpaKioq0a9cuTZo0SSEhIdajmbrxxhv197//Xbm5uXIcR/379y+zz+TJk5Wbm6uioiKtWbNG7du3N5jUlnOprcGDBzvHjx93HnjgAaddu3ZOcnKyU1hY6DRr1sx8tkBZK1eudOLj45327ds7HTt2dJYvX+5kZ2c7derUMZ8tEFdsbKyTlZXlbN++3UlOTjafJ5BWgwYNnN27dzvz5893unXr5rRo0cLp06eP07JlS/PZAmlNmDDB+fbbb5077rjDadGihTNo0CDH7XY7jz32mPlsluu2225zkpKSnIEDBzqO4zj9+/f3+fzYsWOdw4cPOwMHDnQ6dOjgLF682MnNzXXq1atnPns1LvMB/F6bNm1y5s6d67MtIyPDef75581nC9TVsGFDx3Ec58YbbzSfJdBW3bp1nS+//NLp27evs2bNGkJ81po6daqzdu1a8zkCfS1fvtx57bXXfLb99a9/dRYuXGg+W6Cs8kKcl5fnjB071vtxRESEc+jQIWfkyJHm81bXuuSemg4PD1dMTIxWrVrls33VqlXq0aOH0VSBr379+pKkgwcPGk8SeFJSUrRixQqtXr3aepSA1K9fP6WlpWnp0qXKz8/X1q1bNWLECOuxAs6nn36qvn37qnXr1pKkjh07qmfPnnrvvfeMJwtcV155pa644gqfx3OPx6NPPvmkRj2em12PuLIaNmyosLAw5efn+2zPz89XVFSU0VSBb8aMGVq3bp3S09OtRwkoQ4YMUdeuXdWtWzfrUQJWy5YtlZCQoBkzZuj5559X9+7dNXv2bB0/flypqanW4wWM6dOnq379+srMzNTJkycVGhqqiRMn6u2337YeLWCdecwu7/G8RYsWFiOZuORCfIbjOD4fh4SElNmGU15++WXv/53jB02bNtWsWbN066231vhfsfdTatWqpbS0NE2cOFGStH37dnXo0EEJCQmE+EeGDBmi4cOHa9iwYUpPT1fnzp01c+ZM5eXlaeHChdbjBbSa/nh+yYW4oKBAJ06cKHP226hRozL/VwVp9uzZ6tevn3r16qXc3FzrcQJKTEyMGjdurC1btni3hYWFqVevXnr00UdVu3ZtlZaWGk4YGPbt26eMjAyfbTt37tSgQYOMJgpML774oqZNm6YlS5ZIknbs2KEWLVooMTGREJ/D/v37JZ06Mz7z31LNezy/5H5GXFJSoi1btiguLs5ne1xcnDZs2GA0VWCaM2eO7rrrLvXp00fZ2dnW4wSc1atX65prrlHnzp2961//+pcWLVqkzp07E+HT1q9fr7Zt2/psa9OmjXJycowmCkx16tQp83fm5MmTqlXrknuYrTa7d+/Wvn37fB7Pw8PD1bt37xr3eG7+ijF/15m3L913331Ou3btnBkzZjiFhYVO8+bNzWcLlJWSkuIcOnTI6dWrl9O4cWPv+tnPfmY+WyAvXjVddsXGxjoej8dJTEx0WrVq5QwdOtQ5cuSIM2zYMPPZAmktWLDA2bt3r/ftSwMGDHAOHDjgTJs2zXw2y1W3bl2nU6dOTqdOnRzHcZzRo0c7nTp18r7ddOzYsc6hQ4ecAQMGOB06dHAWLVrE25culZWQkODs3r3bOXbsmJOWlsbbcs5a5xIfH28+WyAvQlz++vWvf+188cUXTnFxsZORkeGMGDHCfKZAW/Xq1XOSk5Od7Oxsp6ioyPn666+dpKQkJzw83Hw2y9W7d+9yH4sWLFjg3Wfy5MlOXl6eU1xc7Hz88cdOhw4dzOeuzsVlEAEAMMQPLwAAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADBEiAEAMESIAQAwRIgBADD0/836uuf8hEwKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# animate visited tracks    \n",
    "fig, ax = plt.subplots()\n",
    "image = plt.imshow(track.course, cmap='hot', interpolation='none')\n",
    "time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
    "\n",
    "def get_render_func(_track_maps_l):\n",
    "    def animate(it):\n",
    "        track_map = _track_maps_l[it]\n",
    "        #image.set_array(track.course)\n",
    "        image.set_array(track_map)\n",
    "        time_text.set_text(f\"Iteration {it}\")\n",
    "        return image, time_text\n",
    "    return animate\n",
    "\n",
    "def init():\n",
    "    image.set_array(track.course)\n",
    "    return [image]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, get_render_func(track_maps_l), frames=range(0, len(track_maps_l), 100), \n",
    "                              interval=100, blit=True, init_func=init)\n",
    "ani.save(\"solution_2.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0861c8750a2997ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![SegmentLocal](solution_2.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66a45f80f155ca39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the code block directly below to test the resulting deterministic greedy policy (several samples are taken in order to show behavior in all different starting positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1f0a2326526aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "pos_maps_over_eps_l = []\n",
    "no_episodes = 10\n",
    "for e in range(no_episodes):\n",
    "\n",
    "    _, _, _, _, pos_map = gather_experience(pi, max_episode_len, deterministic=True)\n",
    "    pos_map = (pos_map > 0).astype(np.int16)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    pos_maps_over_eps_l.append(pos_map)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, get_render_func(pos_maps_over_eps_l),\n",
    "                              frames=range(0, len(pos_maps_over_eps_l), 1), \n",
    "                              interval=500, blit=True, init_func=init)\n",
    "ani.save(\"solution_2_2.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70e585406cef8528",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![SegmentLocal](solution_2_2.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
